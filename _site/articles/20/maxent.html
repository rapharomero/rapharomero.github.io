<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Maximum Entropy models for Graphs</title>
  <meta name="description" content="In this post, we give an overview Maximum Entropy models for graphs, as presented in previous work (Bie, 2010) and (Adriaens et al., 2017). We show how these...">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href="//fonts.googleapis.com/css?family=Lato:400,400italic" rel="stylesheet" type="text/css">
  
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
  <script>
    let options = {
      delimiters: [
        { left: "$", right: "$", display: false },
        { left: "$$", right: "$$", display: true },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ]
    }
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, options);
    });

</script>

  


  <link rel="stylesheet" type="text/css" href="/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/css/print.css" media="print"> -->

  <link rel="canonical" href="/articles/20/maxent">

  <link rel="alternate" type="application/rss+xml" title="Raphaël's research website" href="/feed.xml">
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
	<!-- <a href="/"><img class="badge" src="/assets/img/badge_1.png" alt="CH"></a> -->
	
		
  	
		
		    
		      <a href="/about/">About</a>
		    
	    
  	
		
		    
		      <a href="/">Home</a>
		    
	    
  	
		
		    
		      <a href="/css/print.css"></a>
		    
	    
  	
		
  	
	</nav>
</header>
    <article class="group">
      <h1>Maximum Entropy models for Graphs</h1>
<p class="subtitle">October 1, 2020</p>

<p>In this post, we give an overview Maximum Entropy models for graphs, as presented in previous work <a class="citation" href="#debie2010maximum">(Bie, 2010)</a> and <a class="citation" href="#adriaens">(Adriaens et al., 2017)</a>. We show how these models can be used to derive prior distributions on graphs.</p>

<!--more-->

<h3 id="introduction">Introduction</h3>

<p>Many real-world phenomena can be (at least partially) described in the form of networks. Examples include social networks, user behavior online, neurons in the brain, ecological networks etc…</p>

<p>However, while the set of all possible network with a given number of nodes $n$ is very large ($2^{\frac{n(n-1)}{2}}$), the set of real-world networks lie on a very small subset of these, meaning that the <em>majority</em> of possible networks have a negligeable probability of occuring in practice.</p>

<p>While a defining the prior probability of all possible graphs is infeasible due to their huge number, one can easily define prior expectations on the <em>properties</em> of this graph, depending on its nature.</p>

<p>For instance, one might have an idea of the number of links, the number of links <em>per node</em> (their degree).</p>

<p>In a social network, one might have prior expectations about the number of links connecting any two communities. Indeed, the fact that people from the same community tend to connect more than from different ones is a fact commonly observed in real-world social networks and often quoted as <em>homophily</em>.</p>

<p>Based on these prior expactations about the structural properties of the graph, the <em>Maximum Entropy</em> (MaxEnt) principle can be used to cast these expectations into a fully-fledged probability distribution on the combinatorial space of all possible graphs.</p>

<h1 id="formalizing-prior-expectations">Formalizing prior expectations</h1>

<p>In this paragraph, we describe mathematically the aforementioned <em>prior expectations</em>. To do this let’s first introduce some notations.</p>

<h2 id="notations">Notations</h2>

<p>$\newcommand{\Gcal}{\mathcal{G}}$ $\newcommand{\R}{\mathbb{R}}$ $\newcommand{\Gcal}{\mathcal{G}}$ $\newcommand{\Gcal}{\mathcal{G}}$ $\newcommand{\Gcal}{\mathcal{G}}$ $\newcommand{\Gcal}{\mathcal{G}}$Let $U$ a set of nodes of size $n$. A graph is a tuple $G=(U, E)$ where $E\subset U \times U$ is the set of edges of the graph. For a fixed set of nodes $U$, we denote by $\Gcal$ the set of possible undirected graphs connecting the nodes in $U$. Each graph $G\in\Gcal$ can be fully described by its adjacency matrix: $A=(a_{ij})\in \{0,1\}^{n^2}$, such that $a_{ij}=1$ if and only if the nodes $i$ and $j$ are connected. For each node $i \in U$, we denote $\mathcal{N}(i)$ its set of neighbors. We denote by $\newcommand{\PG}{\mathcal{P}(\mathcal{G})}$ $\PG$ the set of graph distributions, i.e. the set of all probability distributions on the set of graphs.</p>

<h2 id="prior-statistics">Prior statistics</h2>

<p>The properties graph can be expressed as <em>graph statistics</em>, which are measurable functions taking as input a graph and yielding a real number:</p>

<p>$$
\begin{align} f&amp;: &amp;G  &amp;\mapsto&amp; &amp;f(G)\\ &amp;&amp;\Gcal &amp;\rightarrow&amp; &amp;\R
\end{align}
$$</p>

<p>Examples of such statistics include for instance:</p>

<ul>
  <li>The degree of each node $i$: $$f_i^{(degree)}(G) = \sum\limits_{j\in \cal{N}(i)} a_{ij}$$</li>
  <li>The number of connections between two node subsets $W, W’ \subset U$: $$f_{W,W’}^{(block)}(G) = \sum\limits_{i,j \in W \times W’} a_{ij}$$</li>
</ul>

<p>As we see, any graph property that can be mathematically computed as a real number can be defined as a graph statistic.</p>

<h2 id="expected-value-of-a-prior-statistics">Expected value of a prior statistics</h2>

<p>$\newcommand{\Ebb}{\mathbb{E}}$ For a given graph distribution $P \in \PG$, and a graph statistic $f$, one can define the expectation of this graph statistic as: $$\Ebb[f(G)]= \sum_{G\in \Gcal} f(G)P(G)$$</p>

<p>This is the mathematical definition of what we mean when we expect a given property about the graph to have a certain value.</p>

<p>The above value allows to compute what a given observer, whose subjectivity is encoded in the prior distribution $P$, expects the graph property $f$ to be.</p>

<!--
\$\$
f: G\in \Gcal \mapsto f(G) \in \R*+
\Gcal \rightarrow \R*+
\$\$ -->

<h1 id="maximum-entropy-models">Maximum Entropy models</h1>

<p>Supposing that we encode our prior expectations into $K$ statistics $f_1,…,f_K$ where each $f_k$ is a real-valued graph function, then the maximum entropy principle can be used to convert those into a <em>prior distribution</em> on the set of possible graphs.</p>

<h2 id="graph-distributions">Graph distributions</h2>

<p>A <em>graph distribution</em> is a probability distribution defined on the set of graphs $\Gcal$. In other words, it can be identified with a function $P$ that gives for each graph $G \in \Gcal$ the likelihood $P(G)$ of observing this particular graph.</p>

<h2 id="entropy-of-a-graph-distribution">Entropy of a graph distribution.</h2>

<p>The <em>entropy value</em> of any distribution $P$ being defined as $$H(P) = -\sum_{G\in\Gcal} P(G)\log(P(G))$$</p>

<p>This quantity measures the average amount of information provided by the observation of a graph, under the distribution $P$.</p>

<p>For instance, if for a given observer all the graphs are equiprobable, the information provided by the observation of a graph is high. In other words this observer will be very <em>surprised</em> on average by the observation.</p>

<p>In contrast, an observer that only gives a non-zero probability to a particular graph $G_0$, and zero probability to all the other graphs, doesn’t get any information when observing a graph sampled from its prior probability $H(P)=0$ in that case.</p>

<!--
Showing a particular graph $\hat{G}$ to an observer having a high entropy prior distribution $P$ will make the latter very _surprised_, in the sense that it will provide him with a lot of information on average.

In contrast, an observer with a low entropy prior, for instance if the observer only expects one graph $\hat{G}$ tohappen  -->

<!-- Under this principle, we want to find a distribution on the set of possible graphs $\Gcal$, that has maximum entropy value, -->

<h2 id="maximizing-the-entropy-under-statistics-based-constraints">Maximizing the entropy under statistics-based constraints</h2>

<p>Supposing that we encode our prior expectations into $K$ statistics $f_1,…,f_K$ where each $f_k$ is a real-valued graph function, then the maximum entropy principle can be used to derive a resulting <em>prior distribution</em> on the set of possible graphs.</p>

<p>While prior expectations about the graph are provided in the form of graph statistics value, we would like to define a distribution over the set of graphs, such that the expected value of the statistics under this distribtution are equal to the one that we expect. In other words we want to impose <em>soft constraints</em> on the graph distribution.</p>

<p>Namely, we want our distribution to satisfy for all $k=1,…,K$: $$\Ebb[f(G)]= c_k$$</p>

<p>where $c_k$ is our prior expectation value for the statistic $k$.</p>

<p>Under these constraints, we use the Maximum Entropy principle to derive the <em>least informative</em> graph prior distribution satisfying the soft constraints.</p>

<p>Achieving this amounts in solving the Maximum Entropy constrained optimization problem:</p>

<!-- % \left\{ -->

<p>$$
\begin{array}{cc}
\max\limits_{P} &amp; H(P) \\ \text{such that}  &amp;\Ebb[f(G)]= c_k , k=1,…,K\\ &amp;\sum_{G\in\Gcal}P(G)=1
\end{array}
$$</p>

<!-- % \right./ -->

<p>It can be shown that the maximum entropy distribution can be written, for a certain parameter vector $\lambda \in \mathbb{R}^K$ and each graph $G\in \mathcal{G}$:</p>

<p>$$
P^*_{\lambda}(G) =
\frac{
\exp(\lambda^T f(G))
}{
\sum_{G \in \mathcal{G}}\exp(\lambda^T f(G))
}
$$</p>

<p>Where $f(G)=(f_1(G), …, f_K(G))$ is the vector of graph statistics.</p>

<h2 id="link-with-maximum-likelihood-estimation">Link with Maximum Likelihood Estimation</h2>

<p>There is a strong connection between the above Maximum Entropy problem and Maximum Likelihood estimation. First we note that these two problems are distinct: while the first is a variational optimization problem (the optimization variable is the probability distribution $P$), the second is an simple convex optimization problem where the optimization variable is the parameter vector $\lambda$.</p>

<p>Their common point is that they are dual problems from each other. Indeed, for any distribution $P$ the Lagrangian associated with the MaxEnt Problem writes:</p>

<p>$$
\begin{aligned}
\mathcal{L}(P, \lambda)
=&amp;-\sum\limits_{G \in \mathcal{G}} P(G) log(P(G))\\ &amp;- \sum\limits_{k=1}^{K} \lambda_k (\sum\limits_{G \in \mathcal{G}} P(G)  f_k(G) -  c_k )
\end{aligned}
$$</p>

<p>$\newcommand{\Lcal}{\mathcal{L}}$ $\newcommand{\Ghat}{\hat{G}}$ $\newcommand{\Pstar}{P^*_{\lambda}}$</p>

<p>In the context of statistics where we observe a graph $\Ghat$ and set $c_k=f_k(\Ghat)$ for all the statistics $k=1,…,K$, it can be easily shown that</p>

<p>$$\Lcal(\Pstar, \lambda) = -\log(\Pstar(\Ghat)).$$ Hence the Lagrangian is exactly equal to the negative log-likelihood of the model.</p>

<h2 id="factorized-form">Factorized form</h2>

<p>A broad range of graph statistics can be decomposed as of edge-specific statistics, i.e.: $\newcommand{\fijk}{f_{ij}^{(k)}}$ $$f_k(G)= \sum\limits_{i \neq j} \fijk(a_{ij}),$$</p>

<p>For instance, the degree of a node is equal to the sum of the corresponding row of the adjacency matrix, and the volume of interaction between two communities is the sum of the entries located in a block of the adjacency matrix.</p>

<p>It can be shown that for these statistics the MaxEnt distribution factorizes over the set of edges. More precisely, in that case we can derive edge-specific statistic vectors, denoted $f_{ij}(G)$, such that:</p>

<p>$$\Pstar(G)=\prod\limits_{i\neq j} P_{ij}(a_{ij})$$ Where for each edge $ij$, $P_{ij}$ is a Bernoulli probability with parameter $$\frac{1}{1+exp(-\lambda^T f_{i,j}(G))}$$ This expression allows to express the graph distribution as a joint distribution of independent edge-specific Bernoulli variables $a_{ij}$. Moreover, the Bernoulli probabilities for each edge are given by a linear logit $\lambda^T f_{i,j}(G)$, passed through the sigmoid function $\sigma :x\mapsto \frac{1}{1+exp(-x)}$.</p>

<h2 id="maxent-in-practice-how-to-turn-prior-knowledge-statistics-into-a-maxent-distribution">MaxEnt in practice: how to turn prior knowledge statistics into a MaxEnt distribution</h2>

<p>In practice, such a distribution can used to extract prior information from an observed graph $\hat{G}$. We recall that the input of this procedure is a set of graph statistic functions, that each quantify an aspect of our expectation on the graph distribution. Based on this, one can apply the statistics $f_k$ to the observed graph, and use the obtained values To do this, one just needs to maximize the above likelihood of the observed graph with respect to the parameter vector $\lambda$:</p>

<p>$$
\begin{aligned}
\max\limits_{\lambda\in \mathbb{R}^K} P(\hat{G}) <br>
\end{aligned}
$$</p>

<p>It can be noted that this Maximum Likelihood problem can be solved using logistic regression. Indeed, for each each edge, we access a feature vector $f_{i,j}(\hat{G})$ use it to predict the presence of absence or link between nodes $i$ and $j$.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We have seen how Maximum Entropy models for graph can be used to formalize prior knowledge about a graph, encoded as soft constraints.</p>

<p>The resulting model has been widely studied in network science literature, under the name of P* (p-star) model, or Exponential random graph models. I</p>

<p>The dyad-independent expression has served as the basis of Later work such as Conditional Network Embeddings <a class="citation" href="#KangLB19">(Kang et al., 2019)</a>.</p>

<h2 id="references">References</h2>

<ol class="bibliography">
<li><span id="debie2010maximum">Bie, T. D. (2010). <i>Maximum entropy models and subjective interestingness: an application to tiles in binary databases</i>.</span></li>
<li><span id="adriaens">Adriaens, F., Lijffijt, J., &amp; De Bie, T. (2017). Subjectively interesting connecting trees. In M. Ceci, J. Hollmén, L. Todorovski, &amp; C. Vens (Eds.), <i>Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2017, Skopje, Macedonia, September 18–22, 2017, Proceedings, Part II</i> (Vol. 10535, Number 2, pp. 53–69). Springer International Publishing. http://dx.doi.org/10.1007/978-3-319-71246-8_4</span></li>
<li><span id="KangLB19">Kang, B., Lijffijt, J., &amp; Bie, T. D. (2019). Conditional Network Embeddings. <i>7th International Conference on Learning Representations, ICLR 2019,
               New Orleans, LA, USA, May 6-9, 2019</i>. https://openreview.net/forum?id=ryepUj0qtX</span></li>
</ol>

<!-- In this paragraph, we have seen how MaxEnt model allow us to encode prior knowledge into a graph distribution $P(G)$ and for a certain type of statistics this translates into a set of independent bernoulli variables with proabilities $P_{ij}(a_{ij})=\sigma(\lambda^Tf_{ij}(G))$.
Now we will see how, once we have derived such a MaxEnt distribution, we can use it to find embeddings conditional on this distribution.

\$\$
\$\$ -->



    </article>
    <span class="print-footer">Maximum Entropy models for Graphs - October 1, 2020 - raphael romero</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="mailto:hate@spam.net"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//plus.google.com/+googlePlusName"><span class="icon-google2"></span></a>
      </li>
    
      <li>
        <a href="//github.com/GithubHandle"><span class="icon-github"></span></a>
      </li>
    
      <li>
        <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr3"></span></a>
      </li>
    
      <li>
        <a href="/feed"><span class="icon-rss2"></span></a>
      </li>
    
      <li>
        <a href="//vimeo.com/vimeo-id"><span class="icon-vimeo2"></span></a>
      </li>
    
      <li>
        <a href="//www.linkedin.com/"><span class="icon-linkedin"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>© 2022   RAPHAEL ROMERO</span> <br>
<span>This site created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme for Content-centric blogging </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>
  </body>
</html>
