<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>A visualization of latent space distance models for Graphs</title>
  <meta name="description" content="">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href="//fonts.googleapis.com/css?family=Lato:400,400italic" rel="stylesheet" type="text/css">
  
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

 <!--   <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
  

  <link rel="stylesheet" type="text/css" href="/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/css/print.css" media="print"> -->

  <link rel="canonical" href="/articles/22/latent-space-viz">

  <link rel="alternate" type="application/rss+xml" title="Raphaël's research website" href="/feed.xml">
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
	<!-- <a href="/"><img class="badge" src="/assets/img/badge_1.png" alt="CH"></a> -->
	
		
  	
		
		    
		      <a href="/about/">About</a>
		    
	    
  	
		
		    
		      <a href="/">Home</a>
		    
	    
  	
		
		    
		      <a href="/css/print.css"></a>
		    
	    
  	
		
  	
	</nav>
</header>
    <article class="group">
      <h1>A visualization of latent space distance models for Graphs</h1>
<p class="subtitle">February 1, 2022</p>

<!--more-->

<h1 id="introduction">Introduction</h1>

<p>Latent space distance models for graphs form a broad class of statistical models for graph. In what follows we give a visual explanation of the mechanisms that allow these models to disantangle different effects in the network generation process.</p>

<h1 id="latent-space-models-for-graphs-lsm">Latent space models for graphs (LSM)</h1>

<p>$\newcommand{\Zcal}{\mathcal{Z}}$ Given a graph $G=(U,E)$ where $U$ is a set of nodes and $E\subset U\times U$ is the set of edges of $G$, a LSM supposes for each node $i\in U$ the existence of an underlying latent representation $z_{i}$ in a metric space $\Zcal$. We denote by $y_{ij}$ the (binary) indicator that the edge $ij$ is in $E$.</p>

<p>Then, supposed that the edges $ij$ in the network are independently generated by a Bernoulli distributions $Bern(\theta_{ij})$, such that: $$\theta_{ij} = f(z_i, z_j)$$ for a certain similarity function $f$.</p>

<p>Examples of Latent Space models include the stochastic block model, where the embeddings are discrete, the graphon model <a class="citation" href="#Lovsz2012LargeNA">(Lovász, 2012)</a>, and the Latent space distance model introduced in <a class="citation" href="#Hoff2002">(Hoff et al., 2002)</a>.</p>

<h1 id="latent-space-distance-models-lsdm">Latent space distance models (LSDM)</h1>

<p>Here we focus on the generic latent space distance models, presented in <a class="citation" href="#Hoff2002">(Hoff et al., 2002)</a>. Those are such that the similarity function is composed by a logit passed through an activation function $h$ (Usually the sigmoid function):</p>

<p>$$a_{ij} \sim Bernoulli(\theta_{ij}) $$
 $$\theta_{ij} = h(2\gamma +\alpha_i + \alpha_j+  \lambda^Tx_{ij} - d(z_i,z_j))$$</p>

<p>Where $\alpha_i$ and $\alpha_j$ <em>sociality</em> parameters, $x_{ij}$ are predefined edge features, $\gamma$ is a bias parameter and $d$ is a distance, such as the euclidean distance.</p>

<!-- While using a similarity measure that is not a distance can also lead to interesting models, here we suppose that $d$ is the euclidean distance -->

<h3 id="deterministic-version-of-the-random-graphs-above">Deterministic version of the random graphs above.</h3>

<p>In order to geometrically explain how LSDMs disantangle, a possible approach is to make the (random) edge Bernoulli random variables, deterministic, by changing the link function. Indeed the sigmoid function is a smooth version of a non-continuous function, the Heaviside step function, given by $h(x) = \mathbb{1}_{{x&gt;0}}$. This one yields an activation equal to 1 for positive inputs and 0 for negative inputs.</p>

<p><img src="/asserts/img/sigmoid_vs_heaviside.png" alt="Heaviside"> <em>The heaviside function in red, and the sigmoid function in green</em></p>

<p>The deterministic graph is given by the following link indicators:</p>

<p>$\newcommand{\ind}{\mathbb{1}}$</p>

<p>$$
y_{ij} = \ind_{ d(z_i,z_j) \leq 2\gamma +\alpha_i + \alpha_j+  \lambda^Tx_{ij}}
$$</p>

<p>This one has a natural visual interpretation, as shown in the following image.</p>

<p>As can be seen, each embedding $z_i$ is endowed with a disk $D_i$ of radius $\alpha_i+\gamma$ such that the minimum distance between $D_i$ and $D_j$ in order for the nodes to connect is $\lambda^T x_{ij}$. If a given node has a large disk, it will naturally form more connections, independent on the position of the disk center.</p>

<p>Moreover the prior similarity between nodes $i$ and $j$ is high, then the disk need not be too close for the connection to form. As a consequence, the embeddings will not encode the prior information contained in the term $\lambda^T x_{ij}$.</p>

<p><img src="/asserts/img/cne_deg1.png" alt="CNE-DEG"></p>

<h2 id="references">References</h2>

<ol class="bibliography">
<li><span id="Lovsz2012LargeNA">Lovász, L. M. (2012). Large Networks and Graph Limits. <i>Colloquium Publications</i>.</span></li>
<li><span id="Hoff2002">Hoff, P. D., Raftery, A. E., &amp; Handcock, M. S. (2002). Latent space approaches to social network analysis. <i>Journal of the American Statistical Association</i>, <i>97</i>(460), 1090–1098. https://doi.org/10.1198/016214502388618906</span></li>
</ol>



    </article>
    <span class="print-footer">A visualization of latent space distance models for Graphs - February 1, 2022 - clay harmon</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="mailto:hate@spam.net"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//plus.google.com/+googlePlusName"><span class="icon-google2"></span></a>
      </li>
    
      <li>
        <a href="//github.com/GithubHandle"><span class="icon-github"></span></a>
      </li>
    
      <li>
        <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr3"></span></a>
      </li>
    
      <li>
        <a href="/feed"><span class="icon-rss2"></span></a>
      </li>
    
      <li>
        <a href="//vimeo.com/vimeo-id"><span class="icon-vimeo2"></span></a>
      </li>
    
      <li>
        <a href="//www.linkedin.com/"><span class="icon-linkedin"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>© 2022   CLAY HARMON</span> <br>
<span>This site created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme for Content-centric blogging </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>
  </body>
</html>
