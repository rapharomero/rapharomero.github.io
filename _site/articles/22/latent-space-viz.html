<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>A visualization of latent space distance models for Graphs</title>
  <meta name="description" content="In this article I give a visualization of latent space distance models for graphs, and how they allow to disantangle the metric structure of the graph from p...">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href="//fonts.googleapis.com/css?family=Lato:400,400italic" rel="stylesheet" type="text/css">
  
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
  <script>
    let options = {
      delimiters: [
        { left: "$", right: "$", display: false },
        { left: "$$", right: "$$", display: true },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ]
    }
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, options);
    });

</script>

  


  <link rel="stylesheet" type="text/css" href="/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/css/print.css" media="print"> -->

  <link rel="canonical" href="/articles/22/latent-space-viz">

  <link rel="alternate" type="application/rss+xml" title="Raphaël's research website" href="/feed.xml">
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
	<!-- <a href="/"><img class="badge" src="/assets/img/badge_1.png" alt="CH"></a> -->
	
		
  	
		
		    
		      <a href="/about/">About</a>
		    
	    
  	
		
		    
		      <a href="/">Home</a>
		    
	    
  	
		
		    
		      <a href="/css/print.css"></a>
		    
	    
  	
		
  	
	</nav>
</header>
    <article class="group">
      <h1>A visualization of latent space distance models for Graphs</h1>
<p class="subtitle">February 1, 2022</p>

<p>In this article I give a visualization of latent space distance models for graphs, and how they allow to disantangle the metric structure of the graph from prior information such as node/edge attributes.</p>

<!--more-->

<h1 id="introduction">Introduction</h1>

<p>Latent space distance models for graphs form a broad class of statistical models for graph. In what follows we give a visual explanation of the mechanisms that allow these models to disantangle different effects in the network generation process.</p>

<h1 id="latent-space-models-for-graphs-lsm">Latent space models for graphs (LSM)</h1>

<p>$\newcommand{\Zcal}{\mathcal{Z}}$ Given a graph $G=(U,E)$ where $U$ is a set of nodes and $E\subset U\times U$ is the set of edges of $G$, a LSM supposes for each node $i\in U$ the existence of an underlying latent representation $z_{i}$ in a metric space $\Zcal$. We denote by $y_{ij}$ the (binary) indicator that the edge $ij$ is in $E$.</p>

<p>Then, supposed that the edges $ij$ in the network are independently generated by a Bernoulli distributions $Bern(\theta_{ij})$, such that: $$\theta_{ij} = f(z_i, z_j)$$ for a certain similarity function $f$.</p>

<p>Examples of Latent Space models include the stochastic block model, where the embeddings are discrete, the graphon model <a class="citation" href="#Lovsz2012LargeNA">(Lovász, 2012)</a>, and the Latent space distance model introduced in <a class="citation" href="#Hoff2002">(Hoff et al., 2002)</a>.</p>

<h1 id="latent-space-distance-models-lsdm">Latent space distance models (LSDM)</h1>

<p>Here we focus on the generic latent space distance models, presented in <a class="citation" href="#Hoff2002">(Hoff et al., 2002)</a>. Those are such that the similarity function is composed by a logit passed through an activation function $h$ (Usually the sigmoid function):</p>

<p>$$a_{ij} \sim Bernoulli(\theta_{ij}) $$
 $$\theta_{ij} = h(2\gamma +\alpha_i + \alpha_j+  \lambda^Tx_{ij} - d(z_i,z_j))$$</p>

<p>Where $\alpha_i$ and $\alpha_j$ <em>sociality</em> parameters, $x_{ij}$ are predefined edge features, $\gamma$ is a bias parameter and $d$ is a distance, such as the euclidean distance.</p>

<!-- While using a similarity measure that is not a distance can also lead to interesting models, here we suppose that $d$ is the euclidean distance -->

<h3 id="deterministic-version-of-the-random-graphs-above">Deterministic version of the random graphs above.</h3>

<p>In order to geometrically explain how LSDMs disantangle, a possible approach is to make the (random) edge Bernoulli random variables, deterministic, by changing the link function. Indeed the sigmoid function is a smooth version of a non-continuous function, the Heaviside step function, given by $h(x) = \mathbb{1}_{{x&gt;0}}$. This one yields an activation equal to 1 for positive inputs and 0 for negative inputs.</p>

<figure><img src="/assets/img/sigmoid_vs_heaviside.png"><figcaption class="maincolumn-figure"><em>The heaviside function in red, and the sigmoid function in green</em></figcaption></figure>

<p>The deterministic graph is given by the following link indicators:</p>

<p>$\newcommand{\ind}{\mathbb{1}}$</p>

<p>$$
y_{ij} = \ind_{ d(z_i,z_j) \leq 2\gamma +\alpha_i + \alpha_j+  \lambda^Tx_{ij}}
$$</p>

<p>This one has a natural visual interpretation, as shown in the following image.</p>

<figure><img src="/assets/img/cne_deg1.png"><figcaption class="maincolumn-figure">Disks associated with each node</figcaption></figure>
<p>As can be seen, each embedding $z_i$ is endowed with a disk $D_i$ of radius $\alpha_i+\gamma$ such that the minimum distance between $D_i$ and $D_j$ in order for the nodes to connect is $\lambda^T x_{ij}$. If a given node has a large disk, it will naturally form more connections, independent on the position of the disk center.</p>

<p>Moreover the prior similarity between nodes $i$ and $j$ is high, then the disk need not be too close for the connection to form. As a consequence, the embeddings will not encode the prior information contained in the term $\lambda^T x_{ij}$.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this article we focus on latent space distance models, and provide a visual interpretation of the mechanism that allow these models to learn vector representation of nodes that do not encode information known in advance in the form of node and edge attributes.</p>

<h2 id="references">References</h2>

<ol class="bibliography">
<li><span id="Lovsz2012LargeNA">Lovász, L. M. (2012). Large Networks and Graph Limits. <i>Colloquium Publications</i>.</span></li>
<li><span id="Hoff2002">Hoff, P. D., Raftery, A. E., &amp; Handcock, M. S. (2002). Latent space approaches to social network analysis. <i>Journal of the American Statistical Association</i>, <i>97</i>(460), 1090–1098. https://doi.org/10.1198/016214502388618906</span></li>
</ol>



    </article>
    <span class="print-footer">A visualization of latent space distance models for Graphs - February 1, 2022 - clay harmon</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="mailto:hate@spam.net"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//plus.google.com/+googlePlusName"><span class="icon-google2"></span></a>
      </li>
    
      <li>
        <a href="//github.com/GithubHandle"><span class="icon-github"></span></a>
      </li>
    
      <li>
        <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr3"></span></a>
      </li>
    
      <li>
        <a href="/feed"><span class="icon-rss2"></span></a>
      </li>
    
      <li>
        <a href="//vimeo.com/vimeo-id"><span class="icon-vimeo2"></span></a>
      </li>
    
      <li>
        <a href="//www.linkedin.com/"><span class="icon-linkedin"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>© 2022   CLAY HARMON</span> <br>
<span>This site created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme for Content-centric blogging </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>
  </body>
</html>
