I"|<!--more-->

<h1 id="introduction">Introduction</h1>

<p>Latent space distance models for graphs form a broad class of statistical models for graph. In what follows we give a visual explanation of the mechanisms that allow these models to disantangle different effects in the network generation process.</p>

<h1 id="latent-space-models-for-graphs-lsm">Latent space models for graphs (LSM)</h1>

<p>$\newcommand{\Zcal}{\mathcal{Z}}$ Given a graph $G=(U,E)$ where $U$ is a set of nodes and $E\subset U\times U$ is the set of edges of $G$, a LSM supposes for each node $i\in U$ the existence of an underlying latent representation $z_{i}$ in a metric space $\Zcal$. We denote by $y_{ij}$ the (binary) indicator that the edge $ij$ is in $E$.</p>

<p>Then, supposed that the edges $ij$ in the network are independently generated by a Bernoulli distributions $Bern(\theta_{ij})$, such that: $$\theta_{ij} = f(z_i, z_j)$$ for a certain similarity function $f$.</p>

<p>Examples of Latent Space models include the stochastic block model, where the embeddings are discrete, the graphon model <a class="citation" href="#Lovsz2012LargeNA">(Lovász, 2012)</a>, and the Latent space distance model introduced in <a class="citation" href="#Hoff2002">(Hoff et al., 2002)</a>.</p>

<h1 id="latent-space-distance-models-lsdm">Latent space distance models (LSDM)</h1>

<p>Here we focus on the generic latent space distance models, presented in <a class="citation" href="#Hoff2002">(Hoff et al., 2002)</a>. Those are such that the similarity function is composed by a logit passed through an activation function $h$ (Usually the sigmoid function):</p>

<p>$$a_{ij} \sim Bernoulli(\theta_{ij}) $$
 $$\theta_{ij} = h(2\gamma +\alpha_i + \alpha_j+  \lambda^Tx_{ij} - d(z_i,z_j))$$</p>

<p>Where $\alpha_i$ and $\alpha_j$ <em>sociality</em> parameters, $x_{ij}$ are predefined edge features, $\gamma$ is a bias parameter and $d$ is a distance, such as the euclidean distance.</p>

<!-- While using a similarity measure that is not a distance can also lead to interesting models, here we suppose that $d$ is the euclidean distance -->

<h3 id="deterministic-version-of-the-random-graphs-above">Deterministic version of the random graphs above.</h3>

<p>In order to geometrically explain how LSDMs disantangle, a possible approach is to make the (random) edge Bernoulli random variables, deterministic, by changing the link function. Indeed the sigmoid function is a smooth version of a non-continuous function, the Heaviside step function, given by $h(x) = \mathbb{1}_{{x&gt;0}}$. This one yields an activation equal to 1 for positive inputs and 0 for negative inputs.</p>

<p><img src="/asserts/img/sigmoid_vs_heaviside.png" alt="Heaviside" /> <em>The heaviside function in red, and the sigmoid function in green</em></p>

<p>The deterministic graph is given by the following link indicators:</p>

<p>$\newcommand{\ind}{\mathbb{1}}$</p>

<p>$$
y_{ij} = \ind_{ d(z_i,z_j) \leq 2\gamma +\alpha_i + \alpha_j+  \lambda^Tx_{ij}}
$$</p>

<p>This one has a natural visual interpretation, as shown in the following image.</p>

<p>As can be seen, each embedding $z_i$ is endowed with a disk $D_i$ of radius $\alpha_i+\gamma$ such that the minimum distance between $D_i$ and $D_j$ in order for the nodes to connect is $\lambda^T x_{ij}$. If a given node has a large disk, it will naturally form more connections, independent on the position of the disk center.</p>

<p>Moreover the prior similarity between nodes $i$ and $j$ is high, then the disk need not be too close for the connection to form. As a consequence, the embeddings will not encode the prior information contained in the term $\lambda^T x_{ij}$.</p>

<p><img src="/asserts/img/cne_deg1.png" alt="CNE-DEG" /></p>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="Lovsz2012LargeNA">Lovász, L. M. (2012). Large Networks and Graph Limits. <i>Colloquium Publications</i>.</span></li>
<li><span id="Hoff2002">Hoff, P. D., Raftery, A. E., &amp; Handcock, M. S. (2002). Latent space approaches to social network analysis. <i>Journal of the American Statistical Association</i>, <i>97</i>(460), 1090–1098. https://doi.org/10.1198/016214502388618906</span></li></ol>
:ET