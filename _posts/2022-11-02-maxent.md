---
layout: post
title: Maximum Entropy (MaxEnt) models for Graphs
author:
  - RaphaÃ«l Romero

header-includes:
  - \usepackage{mathbbm}
bibliography:
  - "bibtex.bib"
date: October 2020
---

<!--more-->

$\newcommand{\Gcal}{\mathcal{G}}$ $\newcommand{\R}{\mathbb{R}}$ $\newcommand{\Gcal}{\mathcal{G}}$ $\newcommand{\Gcal}{\mathcal{G}}$ $\newcommand{\Gcal}{\mathcal{G}}$ $\newcommand{\Gcal}{\mathcal{G}}$

In this post, we explain Maximum Entropy models for graphs, as presented in previous work {% cite debie2010maximum %} and {% cite adriaens %}, and how they can be used to derive prior distributions on graphs.

### Introduction

Many real-world phenomena can be (at least partially) described in the form of networks. Examples include social networks, user behavior online, neurons in the brain, ecological networks etc...

Before any real-world network is observed, the observer generally has prior expectations about the properties of this graph, depending on its nature.

For instance, one might have an idea of the number of links, the number of links _per node_ (their degree).

In a social network, one might have prior expectations about the number of links connecting any two communities. Indeed, the fact that people from the same community tend to connect more than from different ones is a fact commonly observed in real-world social networks and often quoted as _homophily_.

# Formalizing prior expectations

#### Notations

Let $U$ a set of nodes. A graph is a tuple $G=(U, E)$ where $E\subset U \times U$ is the set of edges of the graph. For a fixed set of nodes $U$, we denote by $\Gcal$ the set of possible undirected graphs connecting the nodes in $U$. Each graph $G\in\Gcal$ can be fully described by its adjacency matrix: $A=(a_{ij})\in \\{0,1\\}^{n^2}$, such that $a_{ij}=1$ if and only if the nodes $i$ and $j$ are connected. For each node $i \in U$, we denote $\mathcal{N}(i)$ its set of neighbors.

#### Prior statistics

Prior expectations about a grpah can be expressed or modelled as _statistics_, which are defined as measurable functions taking as input a graph and yielding a real number:

$$
\begin{aligned}
f&: &G  &\mapsto& &f(G)  \\ &&\Gcal &\rightarrow& &\R\\
\end{aligned}
$$

Examples of such statistics include:

- The degree of each node $i$: $$f_i^{(degree)}(G) = \sum\limits_{j\in \cal{N}(i)} a_{ij}$$
- The number of connections between two node subsets $W, W' \subset U$: $$f\_{W,W'}^{(block)}(G) = \sum\limits_{i,j \in W \times W'} a_{ij}$$

<!--
$$
f: G\in \Gcal \mapsto f(G) \in \R*+
\Gcal \rightarrow \R*+
$$ -->

# Maximum Entropy models

Supposing that we encode our prior expectations into $K$ statistics $f_1,...,f_K$ where each $f_k$ is a real-valued graph function, then the maximum entropy principle can be used to convert those into a _prior distribution_ on the set of possible graphs.

#### Graph distributions

A _graph distribution_ is a probability distribution defined on the set of graphs $\Gcal$. In other words, it can be identified witha function $P$ that gives for each graph $G \in \Gcal$ the likelihood $P(G)$ of observing this particular graph

#### Entropy of a graph distribution.

The _entropy value_ of any distribution $P$ being defined as $$H(P) = -\sum_{G\in\Gcal} P(G)\log(P(G))$$

This quantity measures the average amount of information provided by the observation of a graph, under the distribution $P$.

<!-- Under this principle, we want to find a distribution on the set of possible graphs $\Gcal$, that has maximum entropy value, -->

#### Expected value of the prior statistics

$\newcommand{\Ebb}{\mathbb{E}}$ For a given graph distribution $P$, and a graph statistic $f$, one can define the expectation of this graph statistic as: $$\Ebb[f(G)]= \sum_{G\in \Gcal} f(G)P(G)$$

#### Maximizing the entropy under statistics-based constraints

Supposing that we encode our prior expectations into $K$ statistics $f_1,...,f_K$ where each $f_k$ is a real-valued graph function, then the maximum entropy principle can be used to derive a resulting _prior distribution_ on the set of possible graphs.

While prior expectations about the graph are provided in the form of graph statistics value, we would like to define a distribution over the set of graphs, such that the expected value of the statistics under this distribtution are equal to the one that we expect. In other words we want to impose _soft constraints_ on the graph distribution.

Namely, we want our distribution to satisfy for all $k=1,...,K$: $$\Ebb[f(G)]= c_k$$

Where $c_k$ is our prior expectation value for the statistic $k$. Under these constraints, we use the Maximum Entropy principle to define a prior distribution over the set of graphs such that the obtained distribution **provides as least information** as possible, in addition to satisfying the statistic constraints.

Achieving this amounts in solving the Maximum Entropy constrained optimization problem:

<!-- % \left\{ -->

$$
\begin{array}{cc}
\max\limits_{P} & H(P) \\ \text{such that}  &\Ebb[f(G)]= c_k , k=1,...,K\\ &\sum_{G\in\Gcal}P(G)=1
\end{array}
$$

<!-- % \right./ -->

It can be shown that the maximum entropy distribution can be written, for a certain parameter vector $\lambda \in \mathbb{R}^K$ and each graph $G\in \mathcal{G}$:

$$
P^*_{\lambda}(G) =
\frac{
\exp(\lambda^T f(G))
}{
\sum\_{G \in \mathcal{G}}\exp(\lambda^T f(G))
}
$$

Where $f(G)=(f_1(G), ..., f_K(G))$ is the vector of graph statistics.

#### Link with Maximum Likelihood Estimation

There is a strong connection between the above Maximum Entropy problem and Maximum Likelihood estimation. First we note that these two problems are distinct: while the first is a variational optimization problem (the optimization variable is the probability distribution $P$), the second is an simple convex optimization problem where the optimization variable is the parameter vector $\lambda$.

Their common point is that they are dual problems from each other. Indeed, for any distribution $P$ the Lagrangian associated with the MaxEnt Problem writes:

$$
\begin{aligned}
\mathcal{L}(P, \lambda)
=&-\sum\limits_{G \in \mathcal{G}} P(G) log(P(G))\\ &- \sum\limits_{k=1}^{K} \lambda_k (\sum\limits_{G \in \mathcal{G}} P(G)  f_k(G) -  c_k )
\end{aligned}
$$

$\newcommand{\Lcal}{\mathcal{L}}$ $\newcommand{\Ghat}{\hat{G}}$ $\newcommand{\Pstar}{P^*_{\lambda}}$

In the context of statistics where we observe a graph $\Ghat$ and set $c_k=f_k(\Ghat)$ for all the statistics $k=1,...,K$, it can be easily shown that

$$\Lcal(\Pstar, \lambda) = -\log(\Pstar(\Ghat)).$$ Hence the Lagrangian is exactly equal to the negative log-likelihood of the model.

### Factorized form

A broad range of graph statistics can be decomposed as of edge-specific statistics, i.e.: $\newcommand{\fijk}{f_{ij}^{(k)}}$ $$f_k(G)= \sum\limits_{i \neq j} \fijk(a_{ij}),$$

For instance, the degree of a node is equal to the sum of the corresponding row of the adjacency matrix, and the volume of interaction between two communities is the sum of the entries located in a block of the adjacency matrix.

It can be shown that for these statistics the MaxEnt distribution factorizes over the set of edges. More precisely, in that case we can derive edge-specific statistic vectors, denoted $f_{ij}(G)$, such that:

$$\Pstar(G)=\prod\limits_{i\neq j} P_{ij}(a_{ij})$$ Where for each edge $ij$, $P_{ij}$ is a Bernoulli probability with parameter $$\frac{1}{1+exp(-\lambda^T f_{i,j}(G))}$$ This expression allows to express the graph distribution as a joint distribution of independent edge-specific Bernoulli variables $a_{ij}$. Moreover, the Bernoulli probabilities for each edge are given by a linear logit $\lambda^T f_{i,j}(G)$, passed through the sigmoid function $\sigma :x\mapsto \frac{1}{1+exp(-x)}$.

### How to turn prior knowledge statistics into a MaxEnt distribution

In practice, such a distribution can used to extract prior information from an observed graph $\hat{G}$. We recall that the input of this procedure is a set of graph statistic functions, that each quantify an aspect of our expectation on the graph distribution. Based on this, one can apply the statistics $f_k$ to the observed graph, and use the obtained values To do this, one just needs to maximize the above likelihood of the observed graph with respect to the parameter vector $\lambda$:

$$
\begin{aligned}
\max\limits\_{\lambda\in \mathbb{R}^K} P(\hat{G}) \\
\end{aligned}
$$

It can be noted that this Maximum Likelihood problem can be solved using logistic regression. Indeed, for each each edge, we access a feature vector $f_{i,j}(\hat{G})$ use it to predict the presence of absence or link between nodes $i$ and $j$.

### Conclusion

We have seen how Maximum Entropy models for graph can be used to formalize prior knowledge about a graph, encoded as soft constraints.

The resulting model has been widely studied in network science literature, under the name of P\* (p-star) model, or Exponential random graph models. I

The dyad-independent expression has served as the basis of Later work such as Conditional Network Embeddings {% cite KangLB19 %}.

## References

{% bibliography --cited %}

<!-- In this paragraph, we have seen how MaxEnt model allow us to encode prior knowledge into a graph distribution $P(G)$ and for a certain type of statistics this translates into a set of independent bernoulli variables with proabilities $P_{ij}(a_{ij})=\sigma(\lambda^Tf_{ij}(G))$.
Now we will see how, once we have derived such a MaxEnt distribution, we can use it to find embeddings conditional on this distribution.

$$
$$ -->
