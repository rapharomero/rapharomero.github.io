---
title: A visualization of latent space distance models for Graphs
header-includes:
  - \usepackage{bbm}
  - \usepackage{amsmath,amssymb}
layout: post
author:
  - RaphaÃ«l Romero
bibliography:
  - bibtex.bib
date: February 2022
---

# Introduction

Latent space distance models for graphs form a broad class of statistical models for graph.
In what follows we give a visual explanation of the mechanisms that allow these models to disantangle different effects in the network generation process.

# Latent space models for graphs (LSM)

$\newcommand{\Zcal}{\mathcal{Z}}$
Given a graph $G=(U,E)$ where $U$ is a set of nodes and $E\subset U\times U$ is the set of edges of $G$, a LSM supposes for each node $i\in U$ the existence of an underlying latent representation $z_{i}$ in a metric space $\Zcal$.
We denote by $y_{ij}$ the (binary) indicator that the edge $ij$ is in $E$.

Then, supposed that the edges $ij$ in the network are independently generated by a Bernoulli distributions $Bern(\theta_{ij})$, such that:
$$\theta_{ij} = f(z_i, z_j)$$ for a certain similarity function $f$.

Examples of Latent Space models include the stochastic block model, where the embeddings are discrete, the graphon model {% cite Lovsz2012LargeNA %}, and the Latent space distance model introduced in {% cite Hoff2002 %}.

# Latent space distance models (LSDM)

Here we focus on the generic latent space distance models, presented in {% cite Hoff2002 %}. Those are such that the similarity function is composed by a logit passed through an activation function $h$ (Usually the sigmoid function):

$$a_{ij} \sim Bernoulli(\theta_{ij}) $$
 $$\theta_{ij} = h(2\gamma +\alpha_i + \alpha_j+  \lambda^Tx_{ij} - d(z_i,z_j))$$

Where $\alpha_i$ and $\alpha_j$ _sociality_ parameters, $x_{ij}$ are predefined edge features, $\gamma$ is a bias parameter and $d$ is a distance, such as the euclidean distance.

<!-- While using a similarity measure that is not a distance can also lead to interesting models, here we suppose that $d$ is the euclidean distance -->

### Deterministic version of the random graphs above.

In order to geometrically explain how LSDMs disantangle, a possible approach is to make the (random) edge Bernoulli random variables, deterministic, by changing the link function. Indeed the sigmoid function is a smooth version of a non-continuous function, the Heaviside step function, given by $h(x) = \mathbb{1}_{\{x>0\}}$.
This one yields an activation equal to 1 for positive inputs and 0 for negative inputs.

![Heaviside]({{site.url}}/figures/sigmoid_vs_heaviside.png)
_The heaviside function in red, and the sigmoid function in green_

The deterministic graph is given by the following link indicators:
$\newcommand{\ind}{\mathbb{1}}$

$$
y_{ij} = \ind_{ d(z_i,z_j) \leq 2\gamma +\alpha_i + \alpha_j+  \lambda^Tx_{ij}}
$$

This one has a natural visual interpretation, as shown in the following image.

As can be seen, each embedding $z_i$ is endowed with a disk $D_i$ of radius $\alpha_i+\gamma$ such that the minimum distance between $D_i$ and $D_j$ in order for the nodes to connect is $\lambda^T x_{ij}$.
If a given node has a large disk, it will naturally form more connections, independent on the position of the disk center.

Moreover the prior similarity between nodes $i$ and $j$ is high, then the disk need not be too close for the connection to form. As a consequence, the embeddings will not encode the prior information contained in
the term $\lambda^T x_{ij}$.

![CNE-DEG]({{site.url}}/figures/cne_deg1.png)

## References

{% bibliography --cited %}
