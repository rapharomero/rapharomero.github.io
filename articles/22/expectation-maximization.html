<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>ðŸ§® The Expectation-Maximization algorithm</title>
  <meta name="description" content="In this post, I explain the popular Expectation-Maximization algorithm under a variational methods perspective.$\newcommand{\set}[1]{\left\{ #1 \right\}}$">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href="//fonts.googleapis.com/css?family=Lato:400,400italic" rel="stylesheet" type="text/css">
  
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
  <script>
    let options = {
      delimiters: [
        { left: "$", right: "$", display: false },
        { left: "$$", right: "$$", display: true },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ]
    }
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, options);
    });

</script>

  


  <link rel="stylesheet" type="text/css" href="/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/css/print.css" media="print"> -->

  <link rel="canonical" href="/articles/22/expectation-maximization">

  <link rel="alternate" type="application/rss+xml" title="RaphaÃ«l's research website" href="/feed.xml">
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
	<!-- <a href="/"><img class="badge" src="/assets/img/badge_1.png" alt="CH"></a> -->
	
		
  	
		
		    
		      <a href="/">Home</a>
		    
	    
  	
		
		    
		      <a href="/posts">Posts</a>
		    
	    
  	
		
		    
		      <a href="/css/print.css"></a>
		    
	    
  	
		
  	
	</nav>
</header>
    <article class="group">
      <h1>The Expectation-Maximization algorithm</h1>
<p class="subtitle">March 14, 2022</p>

<p>In this post, I explain the popular Expectation-Maximization algorithm under a variational methods perspective.$\newcommand{\set}[1]{\left\{ #1 \right\}}$</p>

<!--more-->

<h1 id="introduction">Introduction</h1>

<p>Letâ€™s consider an observation $x\in\mathcal{X}$. In a traditional statistical setting, we posit a model $\set{ p_{\theta} \vert \theta \in \Theta} $ describing the generation process of the variable $x$, and we would like to make inference on the parameter $\theta$.</p>

<p>However, observations are often best described by introducing an unobserved latent variable $z\in \mathcal{Z}$. This unobserved variable can either correspond to an existing unobserved real-world attribute of the data, or to an abstract hypothesis made to describe the data (for instance in clustering, we define clusters and cluster assignments as latent variables).</p>

<p>In that context, computing the log-likelihood of the data involves integrating over the latent variable, since:</p>

<p>$$
p_{\theta}(x) = \int p_{\theta}(x,z) dz
$$</p>

<h1 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h1>

<p>Letâ€™s suppose we aim to provide a Maximum Likelihood Estimate $\hat{\theta}^{(MLE)}$ of the parameter $\theta$. Classically, to do this we maximize the log-likelihood of the data:</p>

<p>$$
\max\limits_{\theta} \log \left(\int p_{\theta}(x,z) dz \right)
$$</p>

<p>Unfortunately, due to the presence of the integral, this function of $\theta$ is often non-convex, and thus difficult to optimize as such.</p>

<p>However, in many cases, once the latent variable is known, the log-likelihood becomes convex and becomes easy to optimize, <em>for this particular value of the latent variable</em>:</p>

<p>$$\max\limits_{\theta} \log \left(p_{\theta}(x,z) \right)$$</p>

<p>The Expectation-Maximization algorithm <a class="citation" href="#Dempster1977">(Dempster et al., 1977)</a> provides a solution in this particular case, by essentially performing alternate optimization on the parameter, and a variational distribution.</p>

<h1 id="variational-expression-of-the-log-likelihood">Variational Expression of the log-likelihood</h1>

<p>$\newcommand{\PZ}{\mathcal{P(\mathcal{Z})}}$ Let $\PZ$ denote the set of all possible densities defined on the latent space $\mathcal{Z}$.</p>

<p>As is common in variational methods, we use Jensenâ€™s inequality to rewrite the log-likelihood of the data as:</p>

<p>$$
log(p_{\theta}(x)) = \underset{q \in \PZ}{max} \space F(x, q;\theta)
$$</p>

<p>Where $F(x,q; \theta)$ is the <em>variational free energy</em> defined as</p>

<p>$$
F(x, q;\theta) = \int
\log(
  \frac{p_{\theta}(x,z)}{q(z)}
)
  q(z)dz
$$</p>

<p>As a consequence, the maximization of the log likelihood becomes a double maximization problem:</p>

<p>$$
\max \limits_{\theta} \log \left(p_{\theta}(x) \right) = \max\limits_{\theta} \max\limits_{q\in \PZ} F(x,q;\theta)
$$</p>

<h1 id="alternate-optimization-of-the-variational-free-energy">Alternate optimization of the Variational Free Energy</h1>

<p>Since the optimization problem involves two maximizations, the fundamental idea of the EM algorithm is thus to perform alternate optimization.</p>

<p>Supposing that at step $t$ we obtain a parameter value $\theta^{t}$ and $q^t$, we update these values one after the other in two steps</p>

<h3 id="the-expectation-step">The Expectation Step</h3>

<p>In that step, we fix the value of the parameter $\theta^{t}$ and maximize $F$ with respect to the variational distribution:</p>

<p>$$q^{t+1}=\underset{q}{argmax}\space F(x,q,\theta^{t}).$$</p>

<p>Using the Lagrange multiplier methods, one can easily find that the optimal distribution is given by the conditional distribution of the latent variable given the data $x$, under the current set of parameters $\theta^t$:</p>

<p>$$q^{t+1}(z) = p_{\theta^t}(z \vert x)$$</p>

<p>Injecting back this optimal distribution into the variational free energy yields a new convex function of $\theta$, given by:$\newcommand{\E}{\mathbb{E}}$</p>

<p>$$</p>

<p>\begin{aligned}F(x,q^{t+1}, \theta) &amp;=
F(x,p_{\theta^t}(. \vert x), \theta)\\ &amp;=
\int
\log(
  \frac{p_{\theta}(x,z)}{p_{\theta^t}(z \vert x)}
)
  p_{\theta^t}(z \vert x) dz \\ &amp;=
\E_{Z\sim p_{\theta^t}(.\vert x)}[\log(p_{\theta}(x,Z))] + H(p_{\theta^t}(. \vert x))
\end{aligned}
$$</p>

<p>where $H(p_{\theta^t}(. \vert x))$ is the entropy of the conditional latent distribution $p_{\theta^t}(. \vert x)$.</p>

<p>Since the entropy term doesnâ€™t depend on the parameter $\theta$, the annex function to optimize is given by the expectation of the joint likelihood, under the conditional distribution of the latent variable given the observation and the current parameter:</p>

<p>$$
\theta \mapsto E(\theta)=\E_{Z\sim p_{\theta^t}(.\vert x)}[\log(p_{\theta}(x,Z))]
$$</p>

<p>Thus this step is called the <strong>Expectation</strong> step.</p>

<h3 id="the-maximization-step">The Maximization step</h3>

<p>Once the annex function has been derived in the Expectation step, its convexity allows us to easily maximize it with respect to the model parameter:</p>

<p>$$
\theta^{t+1} = \max\limits_{\theta} E(\theta)
$$</p>

<p>This step is thus called the <strong>Maximization step</strong>.</p>

<h1 id="example-the-gaussian-mixture-model">Example: the Gaussian Mixture Model.</h1>

<p>In the Gaussian Mixture Model, we are given a dataset $x={x_1,â€¦,x_n}$, and our goal is to assign a cluster label $c_i\in\{1,â€¦,K\}$ to each datapoint.</p>

<p>To do so, an unobserved cluster assignment variable $z_i \in \{1,â€¦,K\}$ is introduced.</p>

<p>Moreover, each cluster $k\in \{1,â€¦,K\}$ is associated with a Gaussian Distribution $\mathcal{N}(\mu_k, \Sigma_k)$.</p>

<p>Given this, we assume the following generating process for the datapoints:</p>

<p>$$
\begin{aligned}
z_i &amp;\sim \mathcal{M}(1, (\lambda_1,â€¦,\lambda_K)) \\ x_i\vert z_i=k &amp;\sim \mathcal{N}(\mu_k, \Sigma_k)
\end{aligned}
$$</p>

<p>The goal is to find the set of parameters
$\theta=(\lambda_1,â€¦,\lambda_K, \mu_1,â€¦,\mu_K, \Sigma_1,â€¦,\Sigma_K )$ that maximizes the likelihood $ p(x| \theta) $.</p>

<h3 id="e-step">E-step</h3>

<p>As mentionned before, in the E-step we will compute a surrogate function that is the expectation for $z$ distributed under the its conditional distribution given the data and the current set of parameters $\theta^{t}$</p>

<p>Using Bayesâ€™ Rule, we get that this distribution is</p>

<p>$$
\alpha_{i,k}^{t} \overset{\Delta}{=} p(z_i=k|x_i, \theta^{t}) = \frac{\mathcal{N}(x_i;\mu_k^{t}, \Sigma_k^{t})}{\sum_{kâ€™=1}^{K}\mathcal{N}(x_i;\mu_{kâ€™}^{t}, \Sigma_{kâ€™}^{t})}
$$</p>

<p>We deduce that the surrogate function to maximize is</p>

<p>$$E(\theta)= \sum_{i=1}^{n} \sum_{k=1}^{K} \alpha_{k,i}^{t}\left[ log(\lambda_k)+\log(\mathcal{N}(x_i;\mu_k,\Sigma_k)) \right]$$</p>

<p>Note that this maximization has to be done on the set of admissible parameters.
In particular, we should have $\sum_{k=1}^{K}\lambda_k = 1$.</p>

<h3 id="m-step">M-step</h3>

<p>In the M-step, we maximize the surrogate function derived at the E-step, with respect to the model parameters.
In the case of the Gaussian Mixture Model, this has a closed form that writes as follows.</p>

<ul>
  <li>
    <p>Maximizing $E(\theta)$ with respect to $\lambda_1,â€¦,\lambda_K$ under the constraint $\sum_{k=1}^{K}\lambda_k = 1$ yields :</p>

    <p>$$
\lambda_{k}^{t+1} = \frac{1}{n}\sum_{i=1}^{n}\alpha_{i,k}^{t}
$$</p>
  </li>
  <li>
    <p>Maximizing $E(\theta)$ with respect to $\mu_1,â€¦,\mu_{K}$ yields the following re-weighted empirical mean of the data points in each clusters:</p>
  </li>
</ul>

<p>$$
\mu_{k}^{t+1} = \frac{\sum_{i=1}^n\alpha_{i,k}^{t}x_i}{\sum_{i=1}^n\alpha_{i,k}^{t}}
$$</p>

<ul>
  <li>Similarly</li>
</ul>

<p>$$
\Sigma_{k}^{t+1} = \frac{\sum_{i=1}^n\alpha_{i,k}^{t}(x_i-\mu_k^{t+1})(x_i-\mu_k^{t+1})^T}{\sum_{i=1}^n\alpha_{i,k}^{t}}
$$</p>

<h1 id="conclusion">Conclusion</h1>

<p>In this article we have presented the Expectation-Maximization algorithm and its close connection with variational methods. While its name suggest otherwise, this algorithm is simply a form of alternate optimization of the Variational Free Energy, with respect to the model parameters on the one hand, and a variational distribution defined on the latent space on the other hand.</p>

<p>It can be noted that although this algorithm is described in the context where we optimize a log-likelihood, it applies more generally to <em>any setup</em> where the objective function involves integrating over a latent variable.</p>

<ol class="bibliography"><li><span id="Dempster1977">Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977). Maximum Likelihood from Incomplete Data Via the EM Algorithm . In <i>Journal of the Royal Statistical Society: Series B (Methodological)</i> (Vol. 39, Number 1, pp. 1â€“22). https://doi.org/10.1111/j.2517-6161.1977.tb01600.x</span></li></ol>



    </article>
    <span class="print-footer">The Expectation-Maximization algorithm - March 14, 2022 - raphael romero</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="mailto:hate@spam.net"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//plus.google.com/+googlePlusName"><span class="icon-google2"></span></a>
      </li>
    
      <li>
        <a href="//github.com/GithubHandle"><span class="icon-github"></span></a>
      </li>
    
      <li>
        <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr3"></span></a>
      </li>
    
      <li>
        <a href="/feed"><span class="icon-rss2"></span></a>
      </li>
    
      <li>
        <a href="//vimeo.com/vimeo-id"><span class="icon-vimeo2"></span></a>
      </li>
    
      <li>
        <a href="//www.linkedin.com/"><span class="icon-linkedin"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>Â© 2022 Â Â RAPHAEL ROMERO</span> <br>
<span>This site created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme for Content-centric blogging </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>
  </body>
</html>
