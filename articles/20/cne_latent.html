<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Conditional Network Embedding, a Latent Space Distance perspective</title>
  <meta name="description" content="In this article I summarize the Conditional Network Embedding model, and underline its connection with the broader class of Latent Space Distance models for ...">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href="//fonts.googleapis.com/css?family=Lato:400,400italic" rel="stylesheet" type="text/css">
  
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
  <script>
    let options = {
      delimiters: [
        { left: "$", right: "$", display: false },
        { left: "$$", right: "$$", display: true },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ]
    }
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, options);
    });

</script>

  


  <link rel="stylesheet" type="text/css" href="/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/css/print.css" media="print"> -->

  <link rel="canonical" href="/articles/20/cne_latent">

  <link rel="alternate" type="application/rss+xml" title="Raphaël's research website" href="/feed.xml">
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
	<!-- <a href="/"><img class="badge" src="/assets/img/badge_1.png" alt="CH"></a> -->
	
		
  	
		
		    
		      <a href="/about/">About</a>
		    
	    
  	
		
		    
		      <a href="/">Home</a>
		    
	    
  	
		
		    
		      <a href="/css/print.css"></a>
		    
	    
  	
		
  	
	</nav>
</header>
    <article class="group">
      <h1>Conditional Network Embedding, a Latent Space Distance perspective</h1>
<p class="subtitle">November 1, 2020</p>

<p>In this article I summarize the Conditional Network Embedding model, and underline its connection with the broader class of Latent Space Distance models for graphs.</p>

<!--more-->

<h1 id="introduction">Introduction</h1>

<p>Conditional Network Embedding (CNE) <a class="citation" href="#KangLB19">(Kang et al., 2019)</a> is a node embedding method for graphs that has been successfully applied to visualization and prediction. It allows the user to generate node embeddings that respect the network structure, while factoring out prior knowledge known in advance. Applications of this include visualizing the nodes in a network without representing undesired effect, such as for instance having the high degree nodes concentrated in the center of the embedding space. The resulting embeddings can also be used to predict links while controlling the influence of sensitive node attributes on the predictions. This has great interest in producing fair link prediction on social networks, such as in <a class="citation" href="#buyl20a">(Buyl &amp; De Bie, 2020)</a>.</p>

<p>In what follows we aim to give a comprehensive view of the underlying mechanism that make CNE good at producing embeddings that factor out prior information.</p>

<!--

In what follows we express the Conditional Network Embeddings model as a
statistical model for which the parameter space is the cartesian product
of the space of embedding matrices and regression parameters w.r.t. edge
features \$\$f_{ij}\$\$. -->

<h1 id="conditional-network-embedding">Conditional network embedding</h1>

<p>Conditional Network Embedding is a graph embedding method.</p>

<p>Given an undirected graph $G=(U,E)$ where $U$ is the set of nodes and $E\subset U\times U$ is the set of nodes it yields a mapping from the set of nodes to a $d$-dimensional space:</p>

<p>$$
\begin{aligned}
CNE \colon U &amp;\rightarrow &amp; \mathbb{R}^d \\ u &amp;\mapsto &amp; z_u <br>
\end{aligned}
$$</p>

<h1 id="factoring-out-prior-information-in-embeddings">Factoring out prior information in embeddings</h1>

<p>$\newcommand{\norm}[1]{\vert \vert #1 \vert \vert }$ In CNE, we suppose that we have encoded our prior expectations about an observed graph $\hat{G}$ into a MaxEnt distribution(see <a href="//articles/20/maxent">my post about Maxent</a> or the paper <a class="citation" href="#debie2010maximum">(Bie, 2010)</a>). Moreover, we suppose that each node $i \in U$ is represented by an (unknown) embedding vector $z_i \in \mathbb{R}^d$, and that for two nodes $i \neq j$, their connection only depends on the embedding through the euclidean distance between their embeddings $d_{ij} = \norm{z_i-z_j}$.</p>

<p>Based on that, CNE uses Bayes’ rule to define the link probability conditioned on the MaxEnt distribution:</p>

<p>$$
P_{ij}(a_{ij}|z_i, z_j)= \frac{
\mathcal{N}_{+}(d_{ij} | s(a_{ij}))
P_{ij}(a_{i,j})
}{
\sum\limits_{a \in {0,1}}
\mathcal{N}_{+}(d_{ij} | s(a))
P_{ij}(a)
}
$$</p>

<p>where</p>

<ul>
  <li>$d_{ij} = \vert\vert z_i-z_j \vert\vert$ is the euclidean distance between embeddings $z_i$ and $z_j$.</li>
  <li>$\mathcal{N}_{+}(d\vert s(a))$ denotes a half normal density with spread parameter s(a).</li>
  <li>$s$ is a spread function such that $s_0=s(0)&gt;s(1)=s_1$</li>
  <li>$P_{ij}(a)$ is the MaxEnt prior Bernoulli distribution</li>
</ul>

<p>Thus, CNE postulates a distribution over the distance between embeddings, such that the distances between embeddings of non-edges are more spread around 0 than for edges.</p>

<p>Finally, the probability of full graph $G$ is defined as the product of the independent link probabilities:</p>

<p>$$
P(G\vert Z) =\prod_{i\neq j}P_{ij}(a_{ij}|z_i, z_j)
$$</p>

<h3 id="retrieving-the-link-bernoulli-probabilities">Retrieving the link Bernoulli probabilities</h3>

<p>As seen before, the full likelihood of a graph under the CNE model can be written as product of independent probabilities, one for each node pair. As the link indicator $a_{ij}$ between each node pair $ij$ is a Bernoulli random variable, one can transform the expression in order to retrieve the Bernoulli probabilities.</p>

<p>Indeed, it can be shown that the edge link probabilties can be rewritten as: $$P_{ij}(a_{ij} \vert z_i, z_j) =  Q_{ij}^{a_{ij}}(1-Q_{ij})^{(1-a_{ij})}$$</p>

<p>Where:</p>

<ul>
  <li>$Q_{ij} = \sigma \left(\alpha + \lambda^Tf_{ij} - \beta.\frac{d_{ij}^2}{2} \right)$</li>
  <li>$\alpha=\log(\frac{s_1}{s_0})$ is a non-negative constant.</li>
  <li>$\beta=(\frac{1}{s_1^2} - \frac{1}{s_0^2}) \geq 0$ is a scaling constant.</li>
  <li>$\sigma$ still denotes the sigmoid function</li>
</ul>

<h4 id="proof">Proof</h4>

<p>In order to retrieve this form, we first recall the form of the Half-Normal distribution:</p>

<p>$$
\begin{aligned}
p_{\mathcal{N}_{+}(.\vert s)}(d) = \sqrt{\frac{2}{\pi s^2}} exp(- \frac{d^2}{2 d^2})
\end{aligned}
$$</p>

<p>Moreover, the MaxEnt prior distribution writes:</p>

<p>$$
\begin{aligned}
P_{ij}(a_{ij})=\frac{exp(\lambda^Tf_{ij}(G))}{1+exp(\lambda^Tf_{ij}(G))}
\end{aligned}
$$</p>

<p>Since $P_{ij}(a_{ij} \vert z_i, z_j)$ is a Bernoulli probability, we have $Q_{ij} = P_{ij}(1 \vert z_i, z_j)$</p>

<p>Injecting $a_{ij}=1$ in the expression of $P_{ij}(a_{ij} \vert z_i, z_j)$ and simplifying gives:</p>

<p>$$
\begin{aligned}Q_{ij}=&amp; \frac{
\sqrt{\frac{2}{\pi s_1^2}}
exp(- \frac{d_{ij}^2}{2 s_1^2} + \lambda^Tf_{ij})
}{
\sqrt{\frac{2}{\pi s_1^2}}
\exp(- \frac{d_{ij}^2}{2 s_1^2} + \lambda^Tf_{ij}) +
\sqrt{\frac{2}{\pi s_0^2}}
\exp(- \frac{d_{ij}^2}{2 s_0^2})
} \\ = &amp;
\frac{1}{
  1 +
\exp\left(- \frac{d_{ij}^2}{2}(\frac{1}{s_0^2} - \frac{1}{s_1^2}) - \lambda^Tf_{ij} - log(\frac{s_0}{s_1})\right)
} \\ =&amp;
\sigma(\lambda^Tf_{ij} + log(\frac{s_0}{s_1}) - \frac{d_{ij}^2}{2}(\frac{1}{s_1^2} - \frac{1}{s_0^2})) <br>
\end{aligned}
$$</p>

<p>where $\sigma:x \mapsto \frac{1}{1+exp(-x)}$ is the sigmoid function.</p>

<h3 id="connection-with-latent-space-models-for-graphs">Connection with Latent space models for graphs</h3>

<p>As we see, the independent link logits logit in CNE are given by subtracting the scaled distance between embeddings to prior terms and a constant bias: $$logit(Q_{ij})=C+ \lambda^Tf_{ij} - D . d_{ij}^2$$ where $C= log(\frac{s_0}{s_1})$ and $D=0.5*(\frac{1}{s_1^2} - \frac{1}{s_0^2})$</p>

<p>(The logit is defined as the inverse of the sigmoid function: $\sigma(logit(p)) = logit(\sigma(p))=p$)</p>

<p>Intuitively, the term $\lambda^Tf_{ij}$ encodes a prior similarity value between $i$ and $j$ that doesn’t need to be represented by a small distance between the embeddings $z_i$ and $z_j$.</p>

<p>This type of statistical model has been studied in a variety of previous work, in the name of Latent Space Distance Models <a class="citation" href="#Hoff2002">(Hoff et al., 2002; Turnbull &amp; Hons, 2019; Ma et al., 2020)</a>.</p>

<p>The common principle of this type of method is use the latent distance between vector representations as sufficient statistics for the link indicator variable.</p>

<h1 id="example-with-the-degree-and-edge-features-as-prior">Example with the degree and edge features as prior.</h1>

<p>Here we given an example of CNE model where we retrieve the Bernoulli probabilities $Q_{ij}$ given some prior statistics.</p>

<p>We consider a simple example of CNE, where the MaxEnt statistics used are:</p>

<ul>
  <li>
    <p>The degree of each node $i$: $f_i^{(degree)}(G) = \sum\limits_{j\in \cal{N}(i)} a_{ij}$ where $\cal{N}(i)$ is the set of neighbors of $i$. This leads to $n$ statistics at the graph level. For each edge $ij$ the corresponding edge-level statistics vector $f_{ij}$ are given by $[E_i^n \vert\vert E_j^n]$, where for each node $i$, $E_i^n$ is the n-dimensional one-hot encoding of the node $i$ and $\vert\vert$ represents the concatenation operation. Denoting $\alpha \in \mathbb{R}^{2n}$ the vector of coefficients associated to these degree statistics, the corresponding logit value is equal to $$\alpha^Tf_{ij}=\alpha_i + \alpha_j$$</p>
  </li>
  <li>
    <p>Some edge-level features $x_{ij}$. We denote $\theta$ the associated coefficient and the logit values coming from it are equal to : $$\theta^T x_{ij}$$</p>
  </li>
</ul>

<p>So by stacking all these features, we get the following prior term:</p>

<p>$$\lambda^Tf_{ij}=\alpha_i + \alpha_j + \theta^T x_{ij}$$</p>

<p>The CNE Bernoulli probabilities are thus equal to:</p>

<p>$$Q_{ij} = \sigma \left(C + \alpha_i + \alpha_j + \theta^T x_{ij} - D. d_{ij}^2 \right) $$</p>

<!--
# Visual explanation

In order to geometrically explain how CNE factors out prior knowledge, a possible approach is to imagine the (random) edges as Bernoulli random variables, to make them deterministic variables conditioned on the embeddings.

### Deterministic version of the random graphs above.

The sigmoid function is a smooth version of a non-continuous function, the Heaviside step function, given by $h(x) = \mathbb{1}_{\{x>0\}}$.
This one yields an activation equal to 1 for positive inputs and 0 for negative inputs.

![Heaviside](/figures/sigmoid_vs_heaviside.png)
_The heaviside function in red, and the sigmoid function in green_

Let's consider a CNE model, where we use as constraints the degrees of each nodes, as well as other features.
The CNE expression looks like:


\$\$
Q_{ij} = \sigma \left(2 \gamma +\alpha_i + \alpha_j+ \theta^T x_{ij} - \vert\vert z_i-z_j\vert\vert^2 \right)
\$\$

In the deterministic CNE expression, the link indicators would then look like:

\$\$
a_{ij} =h\left(2\gamma +\alpha_i + \alpha_j+ \theta^Tx_{ij} - \vert\vert z_i-z_j\vert\vert \right)
\$\$

This has a natural visual interpretation, as shown in the following image
![CNE-DEG](/figures/cne_deg1.png)

As can be seen, each embedding $z_i$ is endowed with a disk $D_i$of radius $\alpha_i+\gamma$ such that the minimum distance between $D_i$ and $D_j$ in order for the nodes to connect is $\theta^T x_{ij}$.

If the prior similarity is high, the the disk need not be too close for the connection to form. As a consequence, the embeddings will not encode the prior information. -->

<h2 id="conclusion">Conclusion</h2>

<p>We have seen that write the posterior distribution of CNE as a product of Bernoulli distributions, and looking for the Bernoulli parameters allow us to express the CNE model as a Latent Space model for graphs. Such an observation is useful to analyze the theoretical properties (consistency, convergence bounds) of the models, as well as to generalize the approach to different types of graphs (weighted, temporal graphs for instance).</p>

<h2 id="references">References</h2>

<ol class="bibliography">
<li><span id="KangLB19">Kang, B., Lijffijt, J., &amp; Bie, T. D. (2019). Conditional Network Embeddings. <i>7th International Conference on Learning Representations, ICLR 2019,
               New Orleans, LA, USA, May 6-9, 2019</i>. https://openreview.net/forum?id=ryepUj0qtX</span></li>
<li><span id="buyl20a">Buyl, M., &amp; De Bie, T. (2020). DeBayes: a Bayesian Method for Debiasing Network Embeddings. <i>Proceedings of the 37th International Conference on Machine Learning</i>, <i>119</i>, 1220–1229. https://proceedings.mlr.press/v119/buyl20a.html</span></li>
<li><span id="debie2010maximum">Bie, T. D. (2010). <i>Maximum entropy models and subjective interestingness: an application to tiles in binary databases</i>.</span></li>
<li><span id="Hoff2002">Hoff, P. D., Raftery, A. E., &amp; Handcock, M. S. (2002). Latent space approaches to social network analysis. <i>Journal of the American Statistical Association</i>, <i>97</i>(460), 1090–1098. https://doi.org/10.1198/016214502388618906</span></li>
<li><span id="Turnbull2019">Turnbull, K. R., &amp; Hons, M. (2019). <i>Advancements in Latent Space Network Modelling</i>. <i>December</i>.</span></li>
<li><span id="Ma2020a">Ma, Z., Ma, Z., &amp; Yuan, H. (2020). Universal latent space model fitting for large networks with edge covariates. <i>Journal of Machine Learning Research</i>, <i>21</i>, 1–67.</span></li>
</ol>



    </article>
    <span class="print-footer">Conditional Network Embedding, a Latent Space Distance perspective - November 1, 2020 - raphael romero</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="mailto:hate@spam.net"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//plus.google.com/+googlePlusName"><span class="icon-google2"></span></a>
      </li>
    
      <li>
        <a href="//github.com/GithubHandle"><span class="icon-github"></span></a>
      </li>
    
      <li>
        <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr3"></span></a>
      </li>
    
      <li>
        <a href="/feed"><span class="icon-rss2"></span></a>
      </li>
    
      <li>
        <a href="//vimeo.com/vimeo-id"><span class="icon-vimeo2"></span></a>
      </li>
    
      <li>
        <a href="//www.linkedin.com/"><span class="icon-linkedin"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>© 2022   RAPHAEL ROMERO</span> <br>
<span>This site created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme for Content-centric blogging </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>
  </body>
</html>
